{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"color: red;\" >Massih, if you have time you should work through this again with the right data subsets (adding `inplace = True` to the `dropna()` function.  Your code looks good for the most part, but a mistake early on like this has thrown all of your work off!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEFORE STARTING: Rename this notebook - add your _FIRSTNAME_ to the end of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*---------------------------------------------------------------------------------------------------*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data set consists of two .csv files:**\n",
    "- austin_bikeshare_stations.csv  \n",
    "- austin_bikeshare_trips.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "Run the cell below to import the necessary libraries for this project.  \n",
    "  \n",
    "You shouldn't need anything else, but if you really want others, feel free to import more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# pprint is used to pretty-print dictionaries - you can use it if you want\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read & Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Read in both .csv files into two separate pandas data frames. Use the data frame names given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'bike_share/austin_bikeshare_trips.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8e94cc311401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_trips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bike_share/austin_bikeshare_trips.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_stations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bike_share/austin_bikeshare_stations.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'bike_share/austin_bikeshare_trips.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df_trips = pd.read_csv('bike_share/austin_bikeshare_trips.csv')\n",
    "df_stations = pd.read_csv('bike_share/austin_bikeshare_stations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Display the first few lines of data for each data frame  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** How many rows and columns are in each data frame?  \n",
    "- Simply print the values - no need to make a fancy sentence containing the variable... unless you want to.\n",
    "\n",
    "df_trips: 11 cols x 649230 rows\n",
    "\n",
    "df_stations: 6 cols x 72 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"color: red;\" >There are 649231 rows in df_trips - just a typo, I think. You could also call `df_trips.shape` to see rows and columns. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove NaN Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** In an effort to ignore data we don't want to process:  \n",
    "- For both data frames:  \n",
    "    - Drop all rows which contain **ANY** `NaN` values, in any column.  \n",
    "    - DO NOT create new data frames, just drop the rows from the existing data frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"color: red;\" >It is safest to add the `inplace = True` argument when changing a dataframe like this. Your dfs still have the na rows in them.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.dropna(how='any') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** How many rows were dropped from each data frame?\n",
    "\n",
    "df_stations: none\n",
    "\n",
    "df_trips: 649230-581625=67605"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Using the data frame **`df_trips`**  \n",
    "- Make a new list variable and put all the values in the column **`duration_minutes`** into this new list variable.  \n",
    "- Print out the first 10 elements of this list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"color: red;\" >Your code is good, but the numbers are off because you didn't actually drop the rows with na values. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_min = list(df_trips['duration_minutes'])\n",
    "len(dur_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** For each value in this list:  \n",
    "- IF the value falls between: 5 < x < 60 -- Put these values in a **new** list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dur_min_560 = [x for x in dur_min if (x > 5 and x < 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dur_min_560)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** How many values are in this new, filtered, list?  \n",
    "\n",
    "496013\n",
    "<p style = \"color: red;\" >Should be 442783. </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Calculate and print out the average of the values in this new list - **WITHOUT** - using any builtin functions. (i.e. don't use `mean()` or `sum()` )   \n",
    "- ALSO print out the mean using the `mean()` function to verify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot=0\n",
    "for i in range(0,len(dur_min_560)):\n",
    "    tot = tot + dur_min_560[i]\n",
    "    \n",
    "avg = tot / len(dur_min_560)\n",
    "print(avg)\n",
    "print(np.mean(dur_min_560))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Using the original list (NOT the list with the filtered values):  \n",
    "- Multiply each value in this list by 2 - and save the results *back* to the original list. Do NOT save it to a new list.  \n",
    "- Print out the first 3 values of this newly multiplied list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dur_min = [x * 2 for x in dur_min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dur_min[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merge / Drop / Rename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Make a new data frame called **`df_trips_full`** by joining the two data frames: **`df_trips`** and **`df_stations`**  \n",
    "- Merge using the columns: **`start_station_id`** and **`station_id`** respectively.  \n",
    "- *NOTE:* Make sure all the rows from **`df_trips`** are kept in the new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_trips_full = pd.merge(df_trips, df_stations, how='left', left_on='start_station_id', right_on='station_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Display the first 3 rows of this newly merged data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_full.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** From this newly merged data frame(**`df_trips_full`**), drop the columns **`name`** and **`station_id`**, since the information contained in these columns is already contained in other columns in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_trips_full = df_trips_full.drop(['name', 'station_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Because we joined **`df_stations`** to **`df_trips`** using the **`start_station_id`** column, we need to specify that the column called **`status`** which came from the **`df_stations`** is referring to the start_station and not the end_station.  \n",
    "- Therefore: rename the column called **`status`** to **`start_station_id_status`**  \n",
    "- *NOTE:* Don't make a new data frame. Alter the data frame already created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"color: red;\" >Again, `inplace = True` would keep the dataframe with your changes more safely.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_full.rename(index=str, columns={\"status\": \"start_station_id_status\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.** Display the first 3 rows of the data frame **`df_trips_full`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_full.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** How many unique values are there in the collumn called:  **`subscriber_type`** in the data frame **`df_trips_full`** ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"color: red;\" >With the correct subset of the data, this should give you 50. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_trips_full['subscriber_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do the following work with a regex if you can.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** From the column called **`start_time`** in the data frame **`df_trips_full`**:  \n",
    "- Extract the **HOUR** from the starting time of day and put this into a new column called **`checkout_hour`**.  \n",
    "    - The HOUR value that is put into the new column should be of type int() NOT a STRING  \n",
    "    -ALSO:  \n",
    "        - *NOTE* - the time is in the format: `Hours:Minutes:Seconds`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"color: red;\" >Good!! One note. This would run faster if you compile the pattern first.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hrs = df_trips_full['start_time']\n",
    "\n",
    "hr_list=[]\n",
    "for i in range(len(df_trips_full['start_time'])) :\n",
    "    m=re.search('-\\d{2} (\\d{2})',hrs[i])\n",
    "    hr = pd.to_numeric(m.group(1))\n",
    "    hr_list.append(hr)\n",
    "\n",
    "df_trips_full['checkout_hour']=hr_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Display the first few rows to make sure the new column contains the correct contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_full.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~ OPTIONAL ~ ~ ~ :\n",
    "Come back to this OPTIONAL part once you have finished the entire assessment.  \n",
    "**IF** you complete the assignment, **AND** you want some more stuff to do:   \n",
    "- In the data frame **`df_trips_full`** - Convert the column called **`start_time`** to a datetime format, and then extract the **HOUR** into a new column called  **`checkout_hour_datetime`**.   \n",
    "- Note this is doing the exact same thing as the Regex above, but using datetime functionality.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_full['checkout_hour_datetime'] = DateTime.ParseExact(df_trips_full['start_time'], \"yy/MM/dd h:mm:ss tt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Quartiles & Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Using the data frame **`df_trips_full`**: Display the following values for the column: **`duration_minutes`** :\n",
    "- Minimum  \n",
    "- Maximum  \n",
    "- 1st Quartile  \n",
    "- 2nd Quartile  \n",
    "- 3rd Quartile  \n",
    "- Interquartile Range (IQR)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_durmin = df_trips_full['duration_minutes'].max()\n",
    "min_durmin = df_trips_full['duration_minutes'].min()\n",
    "q1_durmin = df_trips_full['duration_minutes'].quantile(.25)\n",
    "q2_durmin = df_trips_full['duration_minutes'].quantile(.5)\n",
    "q3_durmin = df_trips_full['duration_minutes'].quantile(.75)\n",
    "iqr_durmin = q3_durmin - q1_durmin\n",
    "\n",
    "print(\"Max:\", max_durmin)\n",
    "print(\"Min:\", min_durmin)\n",
    "print(\"Q1:\", q1_durmin)\n",
    "print(\"Q2:\", q2_durmin)\n",
    "print(\"Q3:\", q3_durmin)\n",
    "print(\"IQR:\", iqr_durmin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Are there any outliers in the column: **`duration_minutes`**? If so, how many outliers are there in total (above and below)?\n",
    "\n",
    "Nothing below, 147247 above\n",
    "\n",
    "- Outliers here are defined as observations that fall below Q1 − 1.5 IQR or above Q3 + 1.5 IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_min_out = [x for x in dur_min if (x > (q3_durmin + 1.5*iqr_durmin) or x < q1_durmin - 1.5*iqr_durmin)]\n",
    "len(dur_min_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Create a dictionary using the dataframe **`df_trips_full`**.  - Call this dictionary:  **`subscriber_type_dict`**  \n",
    "- The **keys** of the dictionary should be the unique values in the column **`subscriber_type`**   \n",
    "- The **value for each key** should be the number of times each uniuqe subscriber type in **`subscriber_type`** occurs in that column.   \n",
    "    - **NOTE** DO NOT Worry about potential differences in capitalization. i.e. Annual vs annual.    \n",
    "    - Also, Do not worry about separating the counts by year. i.e. there should only be one unique **key** for each subscriber type - not a different key for each year.  \n",
    "    \n",
    "The final dictionary can either have nested keys:  \n",
    "- '7-Day': {'count': 2865}  \n",
    "    - here the subscriber type '7-Day' is the key, which has a nested key called 'count' with the value 2865  \n",
    "    \n",
    "or each key can have only one value and no nested keys, like this:  \n",
    "- '7-Day': 2865\n",
    "    - here the subscriber type '7-Day' is the key, which takes the value: 2865  \n",
    "    \n",
    "Either way is fine.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Display the resulting dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_cnt = df_trips_full['subscriber_type'].value_counts()\n",
    "subscriber_type_dict = uniq_cnt.to_dict()\n",
    "\n",
    "print(subscriber_type_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** How many times did the Founder of Denver's B-Cycle ride? i.e. *`Denver B-cycle Founder`*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriber_type_dict['Denver B-cycle Founder']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~ OPTIONAL ~ ~ ~ :\n",
    "Come back to this OPTIONAL part once you have finished the entire assessment.  \n",
    "**IF** you complete the assignment, **AND** you feel comfortable with this:   \n",
    "- Generalize the dictionary creation process (in part **5a.**) to be a function which will create a dictionary of count occurrences for any column passed in to the function. But by default, process the **`subscriber_type`** column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"color: red;\" >A better function would take a df and a col as arguments. That way it would be useful with other dataframes. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subcnt(x='subscriber_type'):\n",
    "    uniq_cnt = df_trips_full[x].value_counts()\n",
    "    subscriber_type_dict = uniq_cnt.to_dict()\n",
    "    \n",
    "    return subscriber_type_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Dictionary to data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Turn the dictionary you created of subscriber types and number of occurrences (part **5a.**) into a data frame called: **`df_subscriber_freq`**.  \n",
    "- The names of the two columns of the final data frame should be: **`subscriber_type`** and **`number_of_occurrences`**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"color: red;\" >You could convert the dict to a list and then to a df. Passing the `columns=[\"subscriber_type\", \"number_of_occurrences\"]` argument would also give you the requested colnames. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_subscriber_freq = pd.DataFrame(subscriber_type_dict, index=[0])\n",
    "df_subscriber_freq = df_subscriber_freq.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Display the first few rows of the data frame **`df_subscriber_freq`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subscriber_freq.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Which subscriber type had the least number of rides?  How many rides did this subscriber type have?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"color: red;\" >*Which subscriber type*... </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df_subscriber_freq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Which subscriber type had the MOST number of rides?  How many rides did this subscriber type have?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df_subscriber_freq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Plotting using Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Using the dataframe **`df_trips_full`**  \n",
    "- Make a histogram plot of the values in the column **`checkout_hour`** which was created in part **6a**.  \n",
    "- Choose a smart number of bins. Think about this.  \n",
    "- Label the axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"color: red;\" >Since there are 24 distinct hours in a day, `bins = 24` would have been a good choice. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_trips_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d921a64ef5ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchkhr_q1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_trips_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkout_hour'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchkhr_q3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_trips_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkout_hour'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchkhr_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_trips_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkout_hour'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchkhr_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_trips_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'checkout_hour'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_trips_full' is not defined"
     ]
    }
   ],
   "source": [
    "chkhr_q1 = df_trips_full['checkout_hour'].quantile(.25)\n",
    "chkhr_q3 = df_trips_full['checkout_hour'].quantile(.75)\n",
    "chkhr_max = df_trips_full['checkout_hour'].max()\n",
    "chkhr_min = df_trips_full['checkout_hour'].min()\n",
    "\n",
    "chkhr_cnt = len(df_trips_full['checkout_hour'])\n",
    "print(chkhr_q1, chkhr_q3, chkhr_cnt)\n",
    "\n",
    "h = 2 * (chkhr_q3 - chkhr_q1) * (chkhr_cnt ** (-0.333))\n",
    "\n",
    "bincnt = (chkhr_max - chkhr_min) / h\n",
    "\n",
    "print(h, bincnt)\n",
    "\n",
    "_= plt.hist(df_trips_full['checkout_hour'], bins = 24, color = 'blue')\n",
    "_= plt.xlabel = 'Checkout Hours'\n",
    "_= plt.ylabel = 'Frequency'\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
